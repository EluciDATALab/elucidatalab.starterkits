{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/EluciDATALab/elucidatalab.starterkits/main/notebooks/SK_1_2_2_Contextual_Anomaly_Detection/requirements.txt\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starterkits.starterkit_1_2_2.data_handler import fetch_and_unzip_data\n",
    "from starterkits.starterkit_1_2_2.nmf_profiling import extract_nmf_incremental, get_df_W_offline_and_online\n",
    "from starterkits.starterkit_1_2_2.nmf_profiling import get_pivot_table\n",
    "from starterkits.starterkit_1_2_2.visualizations import illustrate_nmf_components_interactive, show_fingerprints, plot_example_interactive\n",
    "from starterkits.starterkit_1_2_2.visualizations import plot_ROC_curve, plot_weights_interactive\n",
    "from starterkits.starterkit_1_2_2.support import get_operating_modes\n",
    "from starterkits.starterkit_1_2_2.preprocessing import get_and_preprocess_healthy_data, get_and_preprocess_unhealthy_data\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Kit 1.2.2: Contextual Performance Profiling and Anomaly Detection\n",
    "\n",
    "## Business context\n",
    "Industry 4.0 leverages on the advanced AI technologies to enable anomaly detection and performance profiling of industrial assets operating in different contexts. Context is defined by both internal and external factors, such as operational conditions, environmental variables, and usage patterns. For this reason, context-aware methods are fundamental to identify anomalies and to ensure accurate and reliable asset profiling. These methods allow for real-time monitoring and enable enhanced performance and reduce downtime of assets. \n",
    "\n",
    "## Business goal\n",
    "\n",
    "The business goal related to this Starter Kit (SK) is to illustrate a data-driven methodology to identify anomalies and profile the performance of assets operating in different contexts, i.e., in terms of process measurements reflecting the internal operations of the asset.\n",
    "As data-driven methodology, this SK focuses on the methodology developed by Fingerhut et al. [[1](#fingerhut2023), [2](fingerhut2024)]. Conventional anomaly detection methods often detect anomalies when the operating conditions change, rendering them less applicable for real-world dynamic industrial use cases. In contrast, the methodology presented in this starterkit is suitable for these scenarios, because it considers the dynamic nature of operating conditions.\n",
    "\n",
    "\n",
    "## Application contexts\n",
    "\n",
    "Contextual anomaly detection and performance profiling play a relevant role in a variety of industrial contexts such as:\n",
    "\n",
    "- Raise warnings to anticipate and avoid safety-critical conditions\n",
    "- Alert the need for inspection to avoid possible downtime and cost corrective maintenance\n",
    "- Performance benchmarking\n",
    "\n",
    "## Data characteristics and requirements\n",
    "To showcase the SK, a dataset is required that includes:\n",
    "\n",
    "- The vibration frequency captured by asset sensors. The captured vibrations need to be related to the health state of the assets.\n",
    "- Parameters related to the internal operations of the asset that influence the vibrations but are not necessarily directly related to the health state.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Kit outline\n",
    "\n",
    "\n",
    "The SK is organized in five main sections. First, the required background knowledge is provided to understand the terminology used in the rest of the document. Second, a description of data generated by the assets along with its preprocessing is reported. Third, the methodology introduced by [Fingerhut et al. [1]](#fingerhut2023) is illustrated highlighting how it can be used for performance profiling. Fourth, the validation of the methodology for anomaly detection is provided. Finally, conclusions are drawn.\n",
    "\n",
    "At the end of the SK you will know how to:\n",
    "\n",
    "- Develop a model for anomaly detection and performance profiling\n",
    "\n",
    "- Experimentally validate the resulting model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "  \n",
    "- **anomaly**: A data point that deviates significantly from what is expected under the given conditions. Anomalies give an indication about potential failures of the asset. \n",
    "- **operating mode**. An asset can operate in different contexts which can influence its behavior. We refer to these context-dependent behaviors as operating modes. As an example we can imagine a gearbox that operates in two modes: normal and throttle. These two modes can be isolated by analzying the context, specifically the rotation speed of the gears. In throttle mode, the gearbox would operate under a reduced load, which would be indicated by a low rotational speed.\n",
    "- **operating and performance views**. The operating view is composed of the parameters capturing the operating context (e.g. rotation speed, torque). The performance view is composed of the parameters monitoring the performance behaviour (e.g. vibrations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data understanding\n",
    "\n",
    "### 2.1 Data description\n",
    "\n",
    "The dataset we will use in this Starter Kit comes from the [PHM North America challenge '23](https://data.phmsociety.org/phm2023-conference-data-challenge/). This dataset collects the time series data from a gearbox subject to pitting, i.e. a fatigue failure of the gear tooth along with metadata. This dataset includes measurements under varied operating conditions, defined in terms of rotational speed and torque, from a healthy state as well as six known fault levels. The training data are collected from a range of different operating conditions under 15 different rotational speeds and 6 different torque levels.  For each operating condition, 5 vibration measurements were collected.\n",
    "The vibration data is given in the time domain with a sampling rate of 20480Hz. The sampling duration differs between 3 and 12 seconds. For each vibration measurement there are tri-axial time-domain vibration measurements available (x, y and z). The vibrations are collected at different rotation per minute (rpm) and different runs.  Below, the user can get acquainted with the dataset by visualizing the vibration measurements in the three directions (x, y and z) for different rpm and runs. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://data.phmsociety.org/wp-content/uploads/sites/9/2023/06/PHM2023dc_fig1.png\" alt=\"MarineGEO circle logo\" style=\"height: 375px; width:800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the data if necessary\n",
    "fetch_and_unzip_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing\n",
    "\n",
    "Splitting the dataset into training and testing sets is a common activity for validating the machine learning models. The training dataset is used to prepare a model, i.e. to train it, whereas the testing dataset is used to evaluate the performance of the model.\n",
    "In our case, the training and the testing set are based on splitting the vibration measurements. More specifically, a random sample of 75% of the original data recorded under normal (= healthy) conditions is used for training whereas the remaining 25% is used for testing.  In the rest of the document, we will only refer to this split to analyze the performance of the methodology for the sake of the computational time. However, to have more reliable results, multiple splits should be performed, i.e. multiple random samples should be extracted for the training and testing sets. The interested reader can refer to [Fingerhut et al. [2]](#fingerhut2024) to see the results when 100 random splits are generated.\n",
    "\n",
    "\n",
    "\n",
    "The testing set is then created by combining:\n",
    "1. **Normal condition**: The 25% healthy data that was held back (not used in the training set)\n",
    "2. **Anomaly condition**: Vibration data characterized by pitting level 1-8 (two more than in the training set). For each level of pitting, there are between 267 and 304 samples in the test set that were recorded at different speeds and torques.\n",
    "\n",
    "The figure below illustrates the train-test split.\n",
    "\n",
    "<img src=\"https://github.com/rpverbek/compact-sk/blob/main/work/figures/overview_train-test-split.png?raw=1\" alt=\"Overview train-test split\" style=\"width:1000px;\"/>\n",
    "\n",
    "Furthermore, from the test set are removed operating conditions (i.e. combinations of rotational speed and torque) which did not appear in the training set (this can happen if during the random train-test split all measurements with the same operating conditions end up only in the test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the next step, the original data in the training set is prepared for the analysis.\n",
    "- The **time series data** is transformed into frequency-bands. Typically, a frequency-band characterizes the vibration behaviour in a specific range of frequencies (measured in Hertz [Hz]). We refer to a frequency-band in terms of \"order\". It is worth to notice that a frequency measure captures the number of events per second whereas the \"order\" captures the number of events per revolution of the rotating element. Finally, an order-transformation is applied to standardize the data. This step is important and a common preprocessing step for prognostics and health management analysis. For details, the interested reader is referred to the vibration alignment section of [Fingerhut et al. [1]](#fingerhut2023). All order-transformed vibration measurements are organized in a matrix which we call the **performance matrix V**.\n",
    "- Metadata is created from the original dataset. The metadata contains the parameters at which the vibrations were measured:\n",
    "    - The **torque** expresses the rotational force in terms of Newton meter.\n",
    "    - The **rotation speed** expresses, how fast the gearbox is rotating in terms of revolutions per minute.\n",
    "  Additionally it contains information about the **measurement direction** (x, y and z) and the **sample ids**, to be able to differentiate between multiple measurements at the same rotation speed and torque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief excerpt of the **performance matrix V** is shown below.\n",
    "\n",
    "In the performance matrix V, all order-transformed vibration measurements are organized in a matrix, where each column contains a frequency band which is two orders long. The exact length of the order-transformed frequency band was determined beforehand based on the smootheness of the signal, but it can be adapted as a hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we need something to showcase how the training and testing dataset looks like (columns, their meaning, etc)\n",
    "df_V_train, meta_data_train, df_data_healthy_test, f = get_and_preprocess_healthy_data()\n",
    "df_orders_test, meta_data_test = get_and_preprocess_unhealthy_data(df_data_healthy_test, f)\n",
    "\n",
    "# extract list of frequency band columns for later usage\n",
    "cols_ = df_V_train.columns\n",
    "BAND_COLS = cols_[cols_.str.contains('band')].tolist()\n",
    "\n",
    "df_V_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix above has n columns which represent vibration measurements and m rows which represent the order-transformed frequency bands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we illustrate the corresponding metadata, consisting of the previously described process parameters `rotational speed [RPM]` and `torque [Nm]`, and some additional information regarding the vibration measurement direction (`direction`). Each row corresponds to the same row in the performance matrix. For instance, the very first measurement contains vibrations recorded at 3000 RPM and 50 NM, for the vibration measurement direction x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_train.drop(columns=['sample_id', 'unique_sample_id']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the use case, a finite number of combinations of speed and torque values are present. These combinations of process parameters, characterizing the behavior of the gearbox, describe the **operating modes** of the asset.\n",
    "<a id='number-of-oms'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_oms = len(meta_data_train['unique_sample_id'].str[:-2].unique())\n",
    "display(md(f'There are {n_oms} unique operating modes.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology\n",
    "\n",
    "The methodology can be divided into the offline and online phases. The offline phase focuses on performance profiling. More specifically, the operating modes of healthy assets are extracted and mapped to the expected performances. The online phase focuses on anomaly detection by exploiting the performance profiles extracted in the previous phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Offline phase\n",
    "\n",
    "The general workflow of the offline phase is reported in the figure below.\n",
    "\n",
    "<img src=\"https://github.com/rpverbek/compact-sk/blob/main/work/figures/overview_offline-phase.png?raw=1\" alt=\"Overview offline phase\" style=\"width:800px;\"/>\n",
    "\n",
    "\n",
    "This phase can be divided into three steps (See Figure above). The main goals of these phases can be summarized as follows:\n",
    "1. Characterizing performance  behaviour. High-dimensional vibration signals are characterized in terms of a few vibration components that capture the fundamental vibration behaviour.\n",
    "2. Extracting operating modes. Based on the process parameters, the operating modes are extracted and associated with each measurement. In the original methodology, a measurement is taken at a specific timestamp. However, in our use case, the gearbox dataset does not contain information about the point in time when a measurement was taken.\n",
    "3. Linking operating modes to performance behaviour. For each operating mode a performance fingerprint is associated.\n",
    "\n",
    "\n",
    "\n",
    "#### Characterizing performance behaviour\n",
    "\n",
    "In this step, we extract for each asset its characteristic **performance behaviour**. These behaviours are extracted by applying a _non-negative matrix factorization_ (NMF) [[3](#nmf)] to the performance matrix V. \n",
    "We apply NMF to decompose the matrix V into a separate weight matrix $\\mathbf{W}$ and a component matrix $\\mathbf{H}$, i.e., $\\mathbf{V} \\approx \\mathbf{W} \\times \\mathbf{H}$. The latter two matrices are lower-dimensional matrices and the approximation contains the essential information about the asset's performance. Hence, NMF reveals underlying patterns in the performance data by representing it in a simpler form.\n",
    "\n",
    "Compared to other decomposition and dimensionality reduction methods (such as PCA or auto-encoders), NMF has the advantage that it decomposes the vibrations into entirely positive values. This is important for interpretability, because non-negative factors tend to be associated with real-world quantities that cannot take negative values, such as the frequency transformed vibrations. As a result, the components in $\\mathbf{H}$ can be understood in combination with the weights in $\\mathbf{W}$ as additive combinations. This makes it easier to understand how the vibrations contribute to the overall performance behavior.\n",
    "    \n",
    "The component matrix $\\mathbf{H}$ contains a set of $h$ representative components allowing to express performance behaviour in a standardized way and thus allowing to compare performance across operating modes and across assets. The matrix $\\mathbf{W}$ contains the weights for reconstructing the original performance matrix $\\mathbf{V}$. Each element in $\\mathbf{W}$ can be interpreted as the weights of the building blocks in $\\mathbf{H}$ needed to reconstruct a vibration signal encoded in the performance matrix $\\mathbf{V}$.\n",
    "\n",
    "To decide the value $h$ of components we decompose the matrix multiple times using an increasing number of components. In the next code section, we apply the NMF to up to 50 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_N_COMPONENTS = 50    # maximum number of components used to recompute\n",
    "\n",
    "df_nmf_models = extract_nmf_incremental(df_V_train, max_n_components=MAX_N_COMPONENTS, timestamps=df_V_train.index, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we illustrate the decomposition of the performance matrix V.\n",
    "\n",
    "The identification of the number of components to describe the operating context is not trivial. To facilitate this process, in the plot below we provide the following plots.\n",
    "- The top two plots visualize how well the performance matrix $\\mathbf{V}$ can be approximated\n",
    "    - The left-top plot illustrates the cumulative explained variance of a principal component analysis (PCA) of the performance matrix $\\mathbf{V}$. It merely serves as an indication of an upper bound for how well the signal could be expressed using PCA.\n",
    "    - The top-righ plot illustrates the reconstruction error of the performance matrix $\\mathbf{V}$ using the NMF components. The reconstruction error is calculated as the Frobenius norm of the difference between the original matrix and the reconstructed matrix.\n",
    "- The bottom lineplots illustrate the NMF components.\n",
    "\n",
    "In the interactive widget below, the reader can select how the number of components are determined. This number can be based on:\n",
    "\n",
    "- a threshold for the cumulative _explained variance_ from PCA (e.g., 95%). In this case, the smallest number of components for which this threshold is exceeded, is selected.\n",
    "- the reconstruction error plot. In this case, the _knee point_ is identified and used to determine the number of components. The identification of this knee point is an extension of the methodology presented in [2] for automated hyperparameter tuning for operative context detection.\n",
    "- both the aforementioned methods. In this case the highest of the two values is used as the number of components.\n",
    "\n",
    "Each of the $h$ components from $\\mathbf{H}$ is illustrated in a separate lineplot. The components serve as building blocks of the observed vibration signals, revealing common patterns in the vibration measurements.\n",
    "\n",
    "For the suggested settings, it can be observed that components 1 and 2 form the basis vectors for peaks that are observed at 40 and 80 orders. These peaks are expected, as the driving gear has 40 teeth. It is common to see peaks for multiples of the number of teeth, hence there is also a peak at 80 orders. Components 3, 4 and 5 encode smaller peaks which model the noise floor.\n",
    "\n",
    "The reader is invited to experiment with multiple parameters below to verify how they affect the number of components extracted. The domain expert can use this code section to incorporate their domain knowledge to make the extraction and representation of implicit contexts more data-efficient .\n",
    "\n",
    "The interactive widget allows to go beyond the analysis performed in the original methodology [[2](#fingerhut2024)]. By increasing the range of acceptable components or by increasing the threshold (in case of using the explained variance as criterium), more components will be used in the decomposition. This makes the vibration fingerprints more granular, but also more prone to noise. In contrast, decreasing the accepted range or the selection criterium will yield less components, making the resulting fingerprints less informative, but also less prone to noise.\n",
    "\n",
    "The number of components should be neither too low, which would lead to inaccurate and too general vibration fingerprints, nor should it be too high, which would lead to too specific fingerprints which could model the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_values = illustrate_nmf_components_interactive(df_V_train, df_nmf_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed with the number of components determined in the interactive figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_COMPONENTS = saved_values['n_components']\n",
    "COMPONENT_COLUMNS = list(range(N_COMPONENTS))  # used later\n",
    "model = df_nmf_models[(df_nmf_models.n_components == N_COMPONENTS)].iloc[0]\n",
    "plural = 's' if N_COMPONENTS > 1 else ''\n",
    "display(md(f\"We are using **{N_COMPONENTS} component{plural}**.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the starterkit is able to cope with different hyperparameters, in the text that follows, we assume that 5 components were chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting operating modes.\n",
    "\n",
    "This step focuses in context-segmentation of context-dependent behaviors of the assets; in other words, this section highlights how the operating modes of the asset are extracted. In this use case, the extraction of the operating modes is straightforward since it maps to the possible combinations of rotational speed and torque.  \n",
    "For clarity, in the text that follows, the operating modes are reported using the pattern `@ X rpm, Y Nm`, where `X` corresponds to the rotational speed and `Y` corresponds to the torque. All measurements with the same rotational speed and torque are summarized in the same operating mode (OM). There are as many operating modes as there are unique combinations of `X` and `Y`.\n",
    "\n",
    "Note that, while in the starter kit, operating modes are extracted independently of the time, it is possible to adapt the operating mode extraction to time-series data, where time spans are assigned a specific operating mode, instead of single measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_operating_modes = get_operating_modes()\n",
    "df_operating_modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pivot table above depicts the names of the extracted operating modes as a function of the two process parameters speed and torque. For example, an asset running in `OM 1` (top left) runs at a rotational speed of 100 rpm and a torque of 50 Nm, whereas an asset running in `OM 77` (bottom right) runs at 1200 rpm and 500 Nm. For this train-test split, there is a total of 77 operating modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='Link-operating-modes-to-performance-behaviour'></a>Linking operating modes to performance\n",
    "In this step, the performance behaviour is linked with the operating modes. This allows to derive **context-sensitive performance fingerprints**. As each vibration measurement is assigned to an operating mode, it is possible to derive fingerprints by aggregating all rows in $\\mathbf{W}$ annotated with the same operating mode.\n",
    "In the following, for each of the 77 operating modes, the reader can visualize the performance behaviour for each individual measurement based on the selected statistics (e.g. mean).\n",
    "Note that for the 77 operating modes, not all of them will have measurements in the training set. Therefore, these operating modes will  not have a fingerprint, which explains why the number of operating modes mentioned at [the end of section 2 ](#number-of-oms) can be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fingerprints(model, df_V_train, meta_data_train, df_operating_modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heat map reports the strength (color) of the weights of the vibration components (cf. NMF component illustration above, x-axis) for the three different directions (y-axis).\n",
    "The weights of the vibration components, which span from 0 to 0.343, are evalueted in terms of power spectral density (PSD).\n",
    "    \n",
    "The PSD is a representation of how the power of the signal is distributed across different frequency components, providing insight into the dominant frequencies present in the vibration data over time.\n",
    "\n",
    "From the analysis of the operating modes, it is possible to observe that they present distinct performance fingerprints, as can be observed for instance when comparing operating mode (OM) 1 with OM 50. Whereas OM 1 predominantly expresses vibrations in the third component related to the noise floor for all vibration directions, OM 50 predominantly expresses vibrations in the first two components related to 40 and 80 orders. At the same time, operating modes with similar operating conditions show similar vibration fingerprints. For instance, OM 1 and OM 16 only differ slightly in torque, which results (as expected) in similar vibration fingerprints.\n",
    "\n",
    "At the end of this offline phase, each operating mode has its own performance profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Online phase\n",
    "\n",
    "The general workflow of the online phase is reported in the figure below. Note that due to the nature of the data used in this use case, some steps are no longer necessary or become very simplified. These steps are still included for the sake of completeness.\n",
    "\n",
    "<img src=\"https://github.com/rpverbek/compact-sk/blob/main/work/figures/overview_online-phase.png?raw=1\" alt=\"Overview online phase\" style=\"width:800px;\"/>\n",
    "\n",
    "This phase can be divided into five steps (See Figure above). The main goals of these phases can be summarized as follows:\n",
    "\n",
    "4. **Windowing incoming streaming**. Streaming data is usually divided into batches. This activity is generally performed to avoid processing each new received data point as it arrives. However, in our use case there is no streaming data, for this reason windowing is not needed and this step is skipped.\n",
    "\n",
    "5. **Detecting of the operating context**. Each timestamp is associated with an operating mode which was identified during the offline phase. In this use case, operating modes are linked to vibration measurements without timestamps.\n",
    "\n",
    "6. **Deriving the performance profiles**. Each vibration measurement is characterized in terms of the vibration components that were extracted in the first step of the offline phase.\n",
    "\n",
    "7. **Estimating the fingerprint offset**. For each vibration measurement, the _offset_ is calculated between the online profiles and the offline fingerprints. The offset quantifies to what extent the observed vibration behaviour differs from the expected vibration behaviour expressed through the fingerprints.\n",
    "\n",
    "8. **Deriving alarms**. In the original paper [[2](#fingerhut2024)], based on the offset, an anomaly score is computed for each timestamp. This score is monitored over time to trigger alarms. In this use case, the monitoring of the anomaly score is not possible since the timestamps in the test set are not ordered. For this reason, this step is skipped.\n",
    "\n",
    "\n",
    "#### Detecting of the operating context and deriving the performance profiles.\n",
    "\n",
    "As in the offline phase, the operating modes are extracted from each timestamp present in the testing dataset. Then, the performance profiles are derived for each vibration measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of extracting the performance profiles is to capture and represent the key characteristics of the vibration signal in a way that reflects the asset's vibration behavior. Therefore, this step essentially mirrors step 1 of the offline phase, with the exception that the vibration components are not derived again.\n",
    "\n",
    "The performance profiles approximate the vibration signal in terms of the representative vibration components that were extracted during step 1 of the offline phase. Similarly to this step, the approximation contains the essential information about the asset's performance.\n",
    "These profiles are extracted by applying NMF to the processed vibration measurements from the online phase, denoted as $\\mathbf{v}'$.\n",
    "The processed measurements $\\mathbf{v}'$ are a vector with the same number of elements as the columns in the performance matrix $\\mathbf{V}$. Since the step is applied to a data stream, multiple samples are no longer stacked together as in $\\mathbf{V}$ during the offline phase.\n",
    "\n",
    "In the online phase, NMF now uses the $h$ components from matrix $\\mathbf{H}$, which were already identified during the offline phase. Specifically, NMF decomposes $\\mathbf{v}'$ into a weight vector $\\mathbf{w}'$ using the fixed component matrix $\\mathbf{H}$, such that $\\mathbf{v}' \\approx \\mathbf{w}' \\times \\mathbf{H}$.\n",
    "\n",
    "Since the same components are used from the offline phase, the profiles match the format of the vibration fingerprints, making them suitable for subsequent anomaly detection steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_W_offline, df_W_online, fingerprints, \n",
    " test_vibration_measurement_periods_meta_data) = get_df_W_offline_and_online(df_V_train, meta_data_train, \n",
    "                                                                             meta_data_test, model, \n",
    "                                                                             df_orders_test, df_operating_modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the derived weights $\\mathbf{w}'$, below the derived weights for a single measurement are illustrated.\n",
    "A vibration measurement consists of the three measurements directions (namely the x-, y- and z-axis, corresponding to the rows in the colormap below).\n",
    "The weights that are illustrated below consist of $h$ columns each, where $h$ represents the number of components determined in the offline phase.\n",
    "The format of the weights is the same as that of the fingerprint shown in the [previous section](#Link-operating-modes-to-performance-behaviour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights_interactive(df_W_online, meta_data_test, df_operating_modes, N_COMPONENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we select measurement 10 from the test set, which was taken at 1000 rpm and 50 Nm and therefore corresponds to operating mode 10. It can be observed that components 1 and 4 have high values in all 3 measurement directions.\n",
    "\n",
    "When comparing this to the vibration fingerprint at OM 10 (see the [interactive widget for showing the fingerprints](#Link-operating-modes-to-performance-behaviour) and choose operating mode 10), we observe a similar behaviour. Hence, for this example, the observed beahviour is similar to the expected vibration behaviour, which is indicative that this vibration measurement can be considered as non-anomalous. This comparison is a representative example of how to check whether the vibration measurement is indicative of an anomaly.\n",
    "\n",
    "At the end of this step, for each online vibration measurement, a performance profile is extracted, which is of the same format as the vibration fingerprints.\n",
    "The performance profile contains the essential information of the vibration signal, while being of the same format as the vibration fingerprints.\n",
    "\n",
    "In the next section, the performance profiles are compared to the expected vibrations from the fingerprints in an automated fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the fingerprint offset\n",
    "\n",
    "In order to assess\n",
    "whether the performance of asset $i$ ($i = 1, . . . , n$) is normal or anomalous, it is necessary to quantify the distance - offset - between the observed and expected fingerprints. For this reason, for each performance measurement, the cosine distance between the derived weights $\\mathbf{w}'$ and the fingerprint $f_{ij} \\in F_i$, corresponding to the detected operating mode $O_{ij}$, ($j = 1, . . . , k_i$), is used to estimate the offset:\n",
    "\n",
    "$$ d_{cos}(\\mathbf{w}_i', \\mathbf{f}_{ij}) = 1 - \\dfrac{\\mathbf{w}_i' \\cdot \\mathbf{f}_{ij}}{\\|\\mathbf{w}_i'\\| \\ \\|\\mathbf{f}_{ij}\\|} $$\n",
    "\n",
    "where || · || is the magnitude of the corresponding vector. In this use case, being present only one asset, $n$=1.\n",
    "\n",
    "The derived weight vectors represent individual direction measurements as a result of decomposing per individual direction. The weight vectors are appended into a single ($3\\times h$)-dimensional vector $\\mathbf{w}_i'$  of rank 1 that is compared to the fingerprint $f_{1j}$ of the corresponding operating mode $O_{1j}$.\n",
    "\n",
    "Once the offsets are extracted, it is possible to evaluate to which extent they can be exploited for anomaly detection. For this reason, in the next code section, for each operating mode the offset of the derived weights to the corresponding fingerprint is computed. More specifically, a pivot table is created and shown below with the cosine distance in order to compare distances between measurements and all operating modes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cosine = get_pivot_table(df_W_online, fingerprints, test_vibration_measurement_periods_meta_data)\n",
    "\n",
    "print(f'Pivot table with distances to all fingerprints, corresponding rpm and torque values, and additional information on the anomaly condition:')\n",
    "display(df_cosine.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "\n",
    "In this section is evaluated to what extent the performance offset, computed at the last step of the methodology, can be exploited for detecting anomalies. In the plot below is analzed the relation between offset distance and pitting level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=df_cosine, y='distance_to_own_cluster_center', x='pitting_level')\n",
    "ax.set_ylabel('Distance to context sensitive fingerprint')\n",
    "ax.set_title(f'Distance to own cluster center per pitting level');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the boxplots above, a clear difference in terms of offset between healthy and faulty gearboxes can be observed. Indeed, healthy gearboxes, the ones with pitting level = 0, have offset close to 0 wherease faulty gearbox, the ones with pitting level above 0, have higher offset. This is expected, as the context-sensitive fingerprint is derived from healthy data without pitting and data with pitting is likely to have vibration patterns not present in the healthy data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot provides only a partial view of the ability of the methodology to identify anomalies. To fully appreciate the performance of the methodology, it is necessary to adequately detect faulty gearboxes as anomalies.\n",
    "Whether a datapoint is labelled as anomalous depends on a predefined distance threshold.\n",
    "If the predefined distance threshold is exceeded, an anomaly is raised.\n",
    "To make our validation independent of the choice of this hyperparameter, we construct a ROC-curve by varying this distance threshold.\n",
    "\n",
    "A ROC curve is a visualisation for evaluating the performance of the anomaly detection. It plots the *true positive rate* (TPR) against the *false positive rate (FPR)* at different distance thresholds. In this use case, the TPR represents the rate of measurements exposed to pitting that were actually detected as anomalies whereas the FPR is the rate of measurements that were not exposed to pitting and still detected as anomaly.\n",
    "The higher the area under the ROC-curve, the better the model is at detecting anomalies.\n",
    "\n",
    "Below the ROC-curve is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC_curve(df_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above it can be observed that the anomaly detection generally performs well with an area under the curve (AUC) of 0.979.\n",
    "Operators aim for a high TPR while minimizing false alarms (keeping the FPR low). Therefore, we additionally tracked the TPR at a stable FPR of 0.1 (TPR@FPR=0.1, red dashed line), which represents the TPR when there are 10% false positives. In this scenario, the TPR would be 96%. An alternative objective criterion is to keep the FPR as low as possible at a high TPR.\n",
    "The plot above therefore also illustrates the FPR@TPR=0.9 (green dashed line).In that scenario, we check how many false alarms would be triggered if we want to guarantee that 90% of the gear pitting is detected. In this use case, FPR@TPR=0.9 is 0.04."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusions\n",
    "\n",
    "In this SK is illustrated a data-driven methodology for contextual performance profiling and anomaly detection. The SK focuses on how to set up methodology parameters and correctly interpret its results. The methodology, validated on a gearbox that is subject to pitting, explains how to extract the contexts from an asset and how to use them to profile its performance. Furthermore, it proves that the performance profiles can be used to identify anomalies. For the latter, it has been shown that our method is able to detect most anomalies, while throwing few false alarms, as demonstrated through the ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional information\n",
    "The methodology presented in this notebook is based on the papers [[1]](#fingerhut2023) and [[2]](#fingerhut2024).\n",
    "\n",
    "This Starter Kit was developed in the context of the Compact project, an enrichment project within the [Flanders AI Research program](https://www.flandersairesearch.be/en). For more information, please contact [info@elucidata.be](mailto:info@elucidata.be).\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Notebook\"), to deal in the Notebook without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Notebook, and to permit persons to whom the Notebook is provided to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies of the Notebook and/or copies of substantial portions of the Notebook.\n",
    "\n",
    "THE NOTEBOOK IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL SIRRIS, THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, DIRECT OR INDIRECT, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE NOTEBOOK OR THE USE OR OTHER DEALINGS IN THE NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "<a id='fingerhut2023'>[1]</a> F. Fingerhut, S. Klein, M. Verbeke, S. Rajendran and E. Tsiporkova, \"Multi-view contextual performance profiling in rotating machinery,\" 2023 IEEE International Conference on Prognostics and Health Management (ICPHM), Montreal, QC, Canada, 2023, pp. 343-350, [doi: 10.1109/ICPHM57936.2023.10194172](https://ieeexplore.ieee.org/document/10194172).\n",
    "\n",
    "<a id='fingerhut2024'>[2]</a> F. Fingerhut, M. Verbeke and E. Tsiporkova, \"Unsupervised context-sensitive anomaly detection on streaming data relying on multi-view profiling,\" 2024 IEEE International Conference on Evolving and Adaptive Intelligent Systems (EAIS), Madrid, Spain, 2024, pp. 1-10, [doi: 10.1109/EAIS58494.2024.10569106](https://ieeexplore.ieee.org/document/10569106).\n",
    "\n",
    "<a id='nmf'>[3]</a> D. Lee, S. Seung, [“Learning the parts of objects by non-negative matrix factorization”](https://www.cs.columbia.edu/~blei/fogm/2020F/readings/LeeSeung1999.pdf), 1999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ©, 2024, Sirris"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
