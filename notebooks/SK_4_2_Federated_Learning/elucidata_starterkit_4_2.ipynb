{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e351c-5d8c-4afe-ac95-038558b09022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import starterkits.starterkit_4_2.support as sp\n",
    "import starterkits.starterkit_4_2.visualizations as vis\n",
    "\n",
    "from pathlib import Path\n",
    "DATA_PATH = Path('../../data/')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a743093-bc0a-436b-b87f-5d841242914b",
   "metadata": {},
   "source": [
    "# Starter Kit 4.2: FedRepo: mitigate concept drift in federated context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d08a3b-c791-40bb-a731-31cbc47039b8",
   "metadata": {},
   "source": [
    "## Passport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b76628-e9b0-4bd6-bd69-19d026783647",
   "metadata": {},
   "source": [
    "### Business context\n",
    "\n",
    "In an increasingly connected world, the concept of federated learning is becoming crucial, especially in scenarios where data privacy is paramount, and bandwidth is a limiting factor. Federated learning enables multiple decentralized devices or servers (clients) to collaboratively learn a predictive model while keeping all training data local. This avoids the need to transfer large volumes of sensitive data to a central server for processing. However, federated learning environments face significant challenges:\n",
    "\n",
    " - Dynamic and heterogeneous data sources often lead to concept drift, where the underlying data patterns the model has learned change over time, potentially degrading the model's performance.\n",
    " - Limited bandwidth for communication between clients and the central server can hinder the efficiency of model updates and retraining processes.\n",
    " - Privacy concerns limit the amount and type of data that can be shared between clients, complicating the detection and mitigation of concept drift.\n",
    "\n",
    "These challenges necessitate advanced strategies for model training and maintenance to ensure that predictive models remain accurate and efficient over time without compromising privacy or incurring prohibitive communication costs.\n",
    "\n",
    "### Business goal\n",
    "\n",
    "The business goal for this Starter Kit is **concept drift mitigation** in federated learning environments. Specifically, this Starter Kit applies a methodology called *FedRepo*, introduced by Tsiporkova et al. [1], that manages a dynamic repository of federated models to effectively cope with concept drift. The FedRepo methodology aims to provide a robust solution that maintains the accuracy and efficiency of the federated models over time, ensuring they adapt to changes in data dynamics while minimizing communication overhead.\n",
    "\n",
    "### Application Context\n",
    "The FedRepo methodology is applicable in various settings where data privacy and limited connectivity are major concerns:\n",
    "\n",
    " - Healthcare: Hospitals and medical institutions can collaborate on developing predictive models or improving diagnostic tools without actually sharing patient data. \n",
    " - Wearables: User experience of several features (e.g. text prediction) can be enhanced on personal devices without compromising privacy.\n",
    " - Industrial: Assets manufactured by a third party (e.g. printers) can be used to collaborately learn predictive models without each customer having to share its data. \n",
    "\n",
    "### Starter Kit outline\n",
    "This Starter Kit will demonstrate the application of the FedRepo methodology using a real-world dataset. First, the dataset will be described, which contains electricity consumption data of UK households. Then, the FedRepo methodology will be explained and discussed through its key steps, while applying them on a subset of the households. For a complete explanation of all steps involved, please refer to the paper. Finally, the performance of the methodology is evaluated, also in terms of its adaptability and concept drift mitigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b74b0-7e64-4dd4-8f69-99f5ceb716ac",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The forecasting of electricity consumption across households is a highly relevant application for this methodology as energy consumption of households obviously is privacy-sensitive. Additionally, many factors could cause for concept drift to occur:\n",
    " - The occupation of the household in terms of its inhabitants\n",
    " - Replacement of household appliances\n",
    " - Seasonal influence on energy consumption\n",
    " - ...\n",
    "\n",
    "The data used is data collected by the UK Power Networks led Low Carbon London project. It consists of 5,567 households in London representing a balanced sample representative of the Greater London population with a 30-minutes granularity between November 2011 and February 2014. The consumption is given in kWh. For demonstrating our methodology, we have randomly selected 300 households for which we have ensured that the data is available until at least 01/2014. For these households, a repository of federated models will be trained in order to forecast the consumption within the next 30 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcb47c7-c325-4f05-81b3-af92dbd261ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "household_subset = sp.get_data(DATA_PATH)\n",
    "household_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22600b1a-a2e3-446e-bc5c-53c47810ceac",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "The next step involves preprocessing the loaded dataset to ensure it's ready for modeling. This includes data cleaning, feature engineering and splitting the data in train and test sets. For this data, a train set of three months is used (January to March 2012) and a test set of one month (April 2012). Features used are the consumption values of up to 6 hours ago, added with the consumption corresponding to same time and 30 minutes before and after on the previous day and week. Additionally, the day of the week, hour of the day and month of the year are also defined as features, with the latter two being cyclically endcoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb260f8a-48d7-412a-88b4-389bc8200743",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = sp.PreprocessConsumption(data=household_subset)\n",
    "\n",
    "x_train, y_train, x_test, y_test = prep.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc8d2d-83a1-42af-8da9-afb46b88588a",
   "metadata": {},
   "source": [
    "## FedRepo\n",
    "The FedRepo algorithm, designed to mitigate concept drift in federated learning environments, is structured around several key steps. These steps ensure that the algorithm dynamically adapts to changes in data distributions across different clients or devices, maintaining the efficacy of the deployed models. FedRepo is built around the maintenance of three repositories residing in a central node, e.g., in the cloud. These are:\n",
    "\n",
    "- $Θ$: a repository of workers, which contains at any moment the workers (clients or devices) for which new federated models need to be constructed.\n",
    "- $Φ$: a repository of global federated random forest models, which contains at any moment the active (deployed) federated models.\n",
    "- $Γ$: a repository of tree models, which contains at any moment subsets of trees from local RF models of each worker.\n",
    "\n",
    "The proposed methodology will continuously update the above described repositories during the use of the federated models based on continuous monitoring and evaluation of the models’ performance. FedRepo consists of four main steps: *Initialization*, *Model training*, *Context-aware inference* and *Dynamic model maintenance*. These are shown in the image below which gives an overview of the methodology. Even though FedRepo is described and evaluated in a regression task scenario, the same methodology can be used for classification by using a proper evaluation metric.\n",
    "\n",
    "<table><tr><td><img src='media/fedrepo.png'><td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a4f0d-f3b2-4fcd-b4bf-0a125c847ecf",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "This step is performed in the central node. The repository of federated RF models is empty since no RF models have been constructed yet, i.e., $Φ = ∅$. Analogously, the workers’ repository contains all available workers since for all of them the federated models still need to be constructed, i.e., $Θ = {θ_{1}, . . . , θ_{300}}$ and the repository of tree models is composed of 300 empty sets, one per worker, i.e., $Γ = {Γ_{1}, . . . , Γ_{300}}$, where $Γi = ∅$, for $i = 1, . . . ,300.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9b4d9-daa7-406e-9568-6871120b2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed = sp.FederatedForest(client_ids=prep.consumer_list)\n",
    "fed.assign_client_data(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d76f08-310d-4fff-a181-5c80b22ae24a",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "In this step, the model and worker repositories are updated such that devices similar with respect to the model performance are assigned to the same cluster of workers and hence collaboratively build and share the same RF federated model. For this, the following steps are executed:\n",
    "\n",
    "1. *Local Model Training*: Each worker trains a local RF model on its training data and selects a subset of trees to contribute to the central repository. The local forests consist of 100 trees per worker.\n",
    "\n",
    "2. *Tree Repository Update*: The central node updates the tree model repository with the trees received from all workers.\n",
    "\n",
    "3. *Federated Global Model Construction*: Construct the initial global RF model $Φ_{0}$ by randomly sampling 100 trees from the updated tree repository. \n",
    "\n",
    "5. *Evaluation Feature Vector Construction*: Each worker evaluates every tree from the global model on its test data, and the performance metrics are used to construct an evaluation feature vector for each worker. The RMSE score is used as the performance metric.\n",
    "\n",
    "These steps will be performed by running the cell below. Note that the communication contents between the local workers and the central node have been: the locally trained trees (to the central node), the global model (to the local workers) and the performance scores (back to the central node). No local data was shared across workers or with the central node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55040e8a-4292-418f-8ed4-a9f033676239",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed.train()\n",
    "fed.evaluate_global_forest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f4ce72-c47b-47c7-a265-0a2fb4296eb0",
   "metadata": {},
   "source": [
    "### Clustering \n",
    "At this stage an evaluation vector of 100 scores exists for each of the 300 workers. These are collected in a matrix which will serve as the basis for a clustering step in which similar workers are grouped together. This grouping is the start of the next few steps in the FedRepo algorithm:\n",
    "\n",
    "5. *Local Node Clustering*: To derive personalized models for a set of similar workers, the workers are split into K non-overlapping clusters. These are obtained by applying the binary PSO algorithm on the evaluation feature vectors calculated in the previous step. The binary PSO clustering has the advantage that the number of clusters does not need to be predefined. In theory, any other clustering method which does not require the number of clusters to be known could have been applied here.\n",
    "\n",
    "6. *Federated Cluster Models Construction*: For each cluster $k$, a federated RF model $Φ_{k}$ is built following the same procedure described in step 3. The trees contributed by all workers in cluster $k$, i.e., the trees contained in the respective $Γ_{i}$ for each worker in the cluster, are pooled together and reshuffled. Subsequently, 100 trees are randomly sampled to create the federated RF model $Φ_{k}$. This model is associated with an initial support score $s_{k}$ which reflects the relative size of the cluster. For example, if cluster $k$ contains 30 workers, than $s_{k}$ = $0.1$.\n",
    "\n",
    "7. *Repository Update*: The repository of federated RF models $Φ$ is extended by adding the newly created federated cluster models. The repository of workers is reset, i.e., $Θ = ∅$, as all workers have an active cluster model deployed for them. The support of the global federated RF model $Φ_{0}$ is also reset to zero, i.e., $s_{0} = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae27a2-ec17-425b-aa0f-712366e73bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_vectors, evaluation_vectors_scaled = fed.get_evaluation_vectors()\n",
    "pso_obj = sp.PSO(samples=evaluation_vectors_scaled, plot_iterations=True)\n",
    "pso_obj.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582bc5a-6ca8-4c50-bdb4-14d083199093",
   "metadata": {},
   "source": [
    "# Still unfinished from here on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a054a86-e0eb-45b7-a62a-8da1f61fd773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08b30e-18f5-407c-b96d-14391a30c1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe896860-9373-48af-a527-256c148f6a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b442e39-632e-41b5-b9f8-bd9c91bb009b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cac38e-22bf-4051-8949-ad83a04ab6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pso_obj.show_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bad0f2-666c-452b-bc32-c1225b4bbc51",
   "metadata": {},
   "source": [
    "### Context-aware inference\n",
    "\n",
    "Each worker receives the parameters of its cluster federated model. At each inference step, each worker calculates the residual between the predicted and observed values for the previous inference step. If the worker’s residual is above a threshold $δ$, that is determined by the model’s performance on the test set, this information is communicated to the central node and the following steps are conducted:\n",
    "\n",
    "1. *Global Model Activation*: The overall federated global model $Φ_{0}$ is activated for the worker in question, i.e., $θi$ is added to $Θ$ and the support of $Φ_{0}$ is updated accordingly, i.e., s0 = s0 +1/M.\n",
    "\n",
    "2. *Model Parameter Update*: The parameters of the corresponding (cluster) federated model (Φk,Θk, sk) are updated, i.e., θi is removed from the list of private workers Θk for this model and the model support is reduced accordingly such that sk = sk − 1/M. Each time the parameters of a federated model in the repository Φ are updated the dynamic model maintenance needs to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04947153-0c84-45f6-b3d8-92332ddc1708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecc0005e-18c0-4160-a4a6-ebf8b0a949d0",
   "metadata": {},
   "source": [
    "### Dynamic model maintenance\n",
    "This final step concerns the identification of federated models in $Φ$ with relatively low support, possibly due to concept drift. Subsequently, a trace of the associated workers is kept in $Θ$ and if this grows above a certain predefined volume $Δ$, training for the affected workers is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d672a6-78d7-41f4-9bff-85c665520c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d1bd8a3-6f2a-401a-9681-3b78e21eebe4",
   "metadata": {},
   "source": [
    "[1] Tsiporkova, E., De Vis, M., Klein, S., Hristoskova, A., & Boeva, V. (2023). Mitigating Concept Drift in Distributed Contexts with Dynamic Repository of Federated Models. In 2023 IEEE International Conference on Big Data (BigData) (pp. 2690-2699). IEEE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa76805e-886f-42bb-ab8a-4fc143ae5c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c9961-70ca-4fde-ae9f-4f730f974bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a07bd79-8a2a-4d0b-8010-8c0ef50ee4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410df1f-0767-4e5f-b1c7-ce1a966e00b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a39ab-c131-4a6e-a67f-43d8fafdd5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starterkit2",
   "language": "python",
   "name": "starterkit2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
